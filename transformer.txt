transformer
    [transformer]

    [[https://ryanong.co.uk/2020/01/09/day-9/][Day 9: Transformers - Introduction - Ryan Ong]]

    Residual connection and layer
    normalisation are applied to each of the
    two sub-layers (Add & Norm).

switch transformer
    https://www.infoq.com/news/2021/02/google-trillion-parameter-ai/

    https://arxiv.org/abs/2101.03961

    $MYGIT/tensorflow/mesh/mesh_tensorflow/transformer/moe.py

temperature
    [#GPT]

    Float value controlling randomness in
    boltzmann distribution.
    
    Lower temperature results in less random
    completions.
    
    As the temperature approaches zero, the
    model will become deterministic and
    repetitive.
    
    Higher temperature results in more random
    completions.

causal masking
    [[https://keras.io/examples/generative/text_generation_with_miniature_gpt/][Text generation with a miniature GPT]]
    [[https://datascience.stackexchange.com/questions/65067/proper-masking-in-the-transformer-model][nlp - Proper masking in the transformer model - Data Science Stack Exchange]]

    Self-attention causality: in the multi-
    head attention blocks used in the decoder,
    this mask is used to force predictions to
    only attend to the tokens at previous
    positions, so that the model can be used
    autoregressively at inference time.
    
    This corresponds to parameter `attn_mask`.

transformer block
block
    The original transformer model is made up
    of an encoder and decoder â€“ each is a
    stack of what we can call transformer
    blocks.

    Both the encoder stack and the decoder
    stack are each made up of consecutive
    transformer blocks.

    http://jalammar.github.io/illustrated-gpt2/

    GPT-2 is built using transformer decoder
    blocks.

    BERT, on the other hand, uses transformer
    encoder blocks

encoder block
    [#transformer block]

decoder block
    [#transformer block]

    Has a small architectural variation from
    the encoder block.

stack
    A stack of transformer blocks.

multi-head attention
    [#transformer]

    [[https://ryanong.co.uk/2020/01/09/day-9/][Day 9: Transformers - Introduction - Ryan Ong]]

    Multi-head Attention is a module for
    attention mechanisms which runs through an
    attention mechanism several times in
    parallel.

    The independent attention outputs are then
    concatenated and linearly transformed into
    the expected dimension.

position-wise feed-forward neural network
position-wise FFNN
    [#transformer]

    [[https://ryanong.co.uk/2020/01/09/day-9/][Day 9: Transformers - Introduction - Ryan Ong]]

    [[https://medium.com/@chiranthancv95/transformers-attention-is-all-you-need-8de139e0fe9e][TransformersAttention is all you need! | by CV CHIRANTHAN | Feb, 2021 | Medium]]

    Each encoder consists of two sub layers:
    - A self-attention layer:
          A layer that helps the encoder look
          at other words in the input sentence
          as it encodes a specific word.
    - An FFNN:
          The exact same FF network is
          independently applied to the word in
          each position through its own path
          in the encoder, hence it is called
          position-wise FF NN.

multi-head self-attention
    [#transformer]

    - 'n' attention heads

    Involves performing multiple single
    attention function (multiple heads), each
    with a different set of query, key and
    value weight matrices.
    
    The purpose of multi-head self-attention
    is that it allows the model to focus on
    information from multiple representation
    subspaces at different positions.
    
    These matrices are initialised randomly
    and are used to project inputs into
    different representation subspace.
    
    We can then perform multiple attention
    function, in parallel, using these
    different representation subspaces
    (queries, keys and values).
    
    Therefore, a transformer with 8 attention
    heads as outlined in Vaswani et al. (2017)
    means that we will end up with 8 different
    output matrices.
    
    Seeing as the FF NN only expects a single
    output matrix, we condense the output
    matrices into a single matrix by
    concatenating them and multiplying it by
    another weight matrix that was also
    trained during the training process.

    See "self-attention".

attention score
    [[https://ryanong.co.uk/2020/01/10/day-10-transformers-multihead-attention-mechanism/][Day 10: Transformers - MultiHead Attention Mechanism - Ryan Ong]]

    Tells the model which part of the sequence
    to focus on when encoding a word.
    
    The score is computed by taking the dot
    products of the query with all the other
    key vectors in the sequence.

representation subspaces
    - queries,
    - keys, and
    - values

head
attention head
    Each with a different set of query, key
    and value weight matrices.    

    See "multi-head self-attention".
